# 多元神经网络
    - 常用激活函数
        - 单层神经网络的案例使用的是softmax激活函数
        - 其他常用激活函数还有：
            - sigmod（S型函数）：x取值在正负无穷之间，y取值范围0-1。当x=0时,y=0.5
            - ReLU（max(0,x）修正线性单元函数）：x取值小于等于0时y=0,x大于零时，y=x(一条直线)
            - tanh（双正切函数）：x取值在正负无穷之间，y取值范围-1到1。当x=0时,y=0

